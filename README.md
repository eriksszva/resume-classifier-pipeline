# Resume Screening Classifier for Data Scientist Applications

![Flow](img/flow.png)

> âš ï¸ *This is a use case example, the final classifier can be integrated with GenAI, this current process does not involve any GenAI components.*

This repository focuses on developing a **scalable and reliable resume classification system** tailored to evaluate the relevance of resumes for data scientist roles. The core implementation emphasizes **machine learning models**, **semantic embeddings**, **API deployment**, and **monitoring infrastructure**, with a strong emphasis on **modular design and long-term maintainability**â€”not on resume extraction or GenAI parsing.

## Data Flow: GenAI â†’ Embeddings â†’ Classifier â†’ User

1. **Input**: A resume (PDF, DOC, or image) is processed using **GenAI summarization or parsing** to extract core sections: `career_objective`, `skills`, `positions`, etc.
2. **Embedding**: The extracted text is passed to a **SentenceTransformer-based embedding API** (`embedder/`) to generate vector representations.
3. **Classification**: These vectors are sent to a **machine learning classifier** (e.g., logistic regression) to determine relevance.
4. **Output**: Results are served back to the user via FastAPI (`resume-api/classifier/`).

## Problems

* **Manual Screening Is Slow:** Reviewing resumes manually is time-intensive, especially with large applicant pools.
* **Inconsistent & Biased Evaluation:** Human judgment is subjective, leading to unfair and inconsistent results.
* **Keyword Matching Misses Context:** Traditional filters fail to capture the actual meaning behind skills and experiences.
* **Too Many Irrelevant Applications:** Most applicants donâ€™t meet data science criteria, wasting valuable screening effort.

## ğŸ—‚ï¸ Project Structure: Machine Learning Development & Monitoring-Logging System

```
â”œâ”€â”€ ğŸ“„ Experiment_SML.txt           # Link to GitHub repo for data preprocessing  
â”œâ”€â”€ ğŸ“„ Workflow-CI.txt              # Link to GitHub repo for data CI/CD workflow  

â”œâ”€â”€ ğŸ“ Membangun_model/             # Model development, training, tuning, and experiment tracking  
â”‚   â”œâ”€â”€ ğŸ“ Artifacts/                   # Output artifacts from MLflow runs  
â”‚   â”œâ”€â”€ ğŸ“ cache/                       # Cached embeddings data during experiments  
â”‚   â”œâ”€â”€ ğŸ“ cleaned-data/                # Cleaned dataset for training  
â”‚   â”œâ”€â”€ ğŸ“ screenshots/                 # Visual evidence of saved artifacts and dashboards  
â”‚   â”œâ”€â”€ ğŸ“ utils/                       # Helper modules for embedding and MLflow environment tracking  
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ PipelineWrapperModel.py     # Wrapper for scikit-learn pipeline with SentenceTransformer for MLflow compatibility  
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ SentenceTransformers.py     # Embedding utility using SentenceTransformer with batching logic  
â”‚   â”‚   â””â”€â”€ ğŸ“„ env.yaml                    # Custom conda environment to ensure correct dependencies during model logging  
â”‚   â”œâ”€â”€ ğŸ“„ DagsHub.txt                  # Link to the DagsHub project page  
â”‚   â”œâ”€â”€ ğŸ“„ modelling.py                 # ML model training script with MLflow autologging  
â”‚   â”œâ”€â”€ ğŸ“„ modelling_tuning.py          # Hyperparameter tuning with manual MLflow logging  
â”‚   â””â”€â”€ ğŸ“„ requirements.txt             # Python dependencies for modeling  

â”œâ”€â”€ ğŸ“ Monitoring_dan_Logging/       # Monitoring, logging, alerting, and model API serving  
â”‚   â”œâ”€â”€ ğŸ“ config/  
â”‚   â”‚   â””â”€â”€ ğŸ“„ prometheus.yaml              # Prometheus configuration file  
â”‚   â”œâ”€â”€ ğŸ“ docker/  
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ docker-compose-model.yaml       # Docker Compose file for model services  
â”‚   â”‚   â””â”€â”€ ğŸ“„ docker-compose-monitoring.yaml  # Docker Compose file for Prometheus & Grafana stack  
â”‚   â”œâ”€â”€ ğŸ“ evidence/  
â”‚   â”‚   â”œâ”€â”€ ğŸ“ alerting-grafana/            # Screenshots of Grafana alert configuration and results  
â”‚   â”‚   â”œâ”€â”€ ğŸ“ model-serving/               # Proof of successful model serving  
â”‚   â”‚   â”œâ”€â”€ ğŸ“ monitoring-grafana/          # Grafana dashboard visualizations  
â”‚   â”‚   â””â”€â”€ ğŸ“ monitoring-prometheus/       # Prometheus metric interface screenshots  
â”‚   â”œâ”€â”€ ğŸ“ exporter/  
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile                   # Dockerfile for Prometheus metrics exporter  
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ requirements.txt             # Python dependencies for exporter service  
â”‚   â”‚   â””â”€â”€ ğŸ“„ exporter.py                  # Flask app exposing metrics to Prometheus  
â”‚   â”œâ”€â”€ ğŸ“ inference/  
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ inference.py                 # Script to send requests to the model API  
â”‚   â”‚   â””â”€â”€ ğŸ“„ serving_input_example.json   # Sample input for model inference  
â”‚   â”œâ”€â”€ ğŸ“ model/  
â”‚   â”‚   â””â”€â”€ ğŸ“„ logreg_model.py              # Logistic Regression model script for API usage  
â”‚   â””â”€â”€ ğŸ“ resume-api/  
â”‚       â”œâ”€â”€ ğŸ“ classifier/                  # ML classifier API served via Docker (e.g., FastAPI or Flask)  
â”‚       â”‚   â”œâ”€â”€ ğŸ“ app/  
â”‚       â”‚   â”‚   â”œâ”€â”€ ğŸ“„ main.py                  # Main application logic (predict endpoint)  
â”‚       â”‚   â”‚   â”œâ”€â”€ ğŸ“„ model.joblib             # Serialized ML model  
â”‚       â”‚   â”‚   â””â”€â”€ ğŸ“„ schema.py                # Pydantic schema for input validation  
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ .dockerignore               # Docker ignore rules  
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile                  # Dockerfile to containerize the classifier app  
â”‚       â”‚   â””â”€â”€ ğŸ“„ requirements.txt            # Dependencies for the classifier service  
â”‚       â””â”€â”€ ğŸ“ embedder/                   # Text embedding API running locally (not containerized)  
â”‚           â”œâ”€â”€ ğŸ“ app/  
â”‚           â”‚   â”œâ”€â”€ ğŸ“„ main.py                  # Main logic for embedding endpoint  
â”‚           â”‚   â””â”€â”€ ğŸ“„ schema.py                # Input schema for embedding requests  
â”‚           â””â”€â”€ ğŸ“„ requirements.txt            # Dependencies for the embedding service  
```

## Modular Architecture for Separation of Concerns

The project is structured into **clearly separated modules**, each responsible for a distinct function:

1. **Model Development** (`Membangun_model/`)
2. **Monitoring and Logging System** (`Monitoring_dan_Logging/`)
3. **CI/CD Pipeline** (`Workflow-CI.txt`)
4. **Data Preprocessing Experiment** (`Experiment_SML.txt`)

## Semantic Understanding via Sentence Embeddings

A key innovation in this system is the use of **pre-trained Sentence Transformers** to capture **semantic meaning** from resumes, instead of relying on keyword matching or traditional bag-of-words techniques.

This approach allows the classifier to understand the *context* and *intent* behind phrases such as:

* `"Built machine learning pipelines"` â†’ semantically similar to â†’ `"Designed end-to-end ML workflows"`

By embedding each resume into high-dimensional vector space using models like `all-MiniLM-L6-v2`, the system can generalize across varying terminology while retaining nuanced meaning â€” crucial for real-world resume screening.

## Embedding API as a Standalone Service

![embedder](Monitoring_dan_Logging/evidence/model-serving/embedding-endpoint.png)

To keep the main Docker image lightweight and scalable, the Sentence Transformers embedding service is run **outside the container** (locally). 

> âš ï¸ Running this inside Docker would require downloading large dependencies such as PyTorch, Transformers, and pretrained model weights (~500MB+), which would significantly increase image size, slow down container startup, and create bottlenecks in autoscaled environments.

This design enables:
* **Loose Coupling** â€“ Embeddings can be reused across multiple downstream tasks (e.g., classification, retrieval, similarity search).
* **Flexibility** â€“ Embedding models can be swapped or upgraded independently without changing the classifier or main pipeline logic.

## Embedding Caching for Performance Optimization

The `cache/` directory stores previously computed embeddings, which improves system **efficiency** and **latency** during repeated training.

* Avoids redundant computation.
* Ensures deterministic results during development.
* Accelerates batch inference for large-scale datasets.

## Monitoring & Logging: Best Practices for Model Health

![dashboard-grafana](Monitoring_dan_Logging/evidence/monitoring-grafana/overall-dashboard(last-6hours).png)

The `Monitoring_dan_Logging/` module implements a **full observability stack** using:

* **Prometheus**: For collecting metrics (e.g., request rate, latency, error counts).
* **Grafana**: For real-time dashboards and visual monitoring.
* **Custom Exporter**: A lightweight Flask app (`prometheus_exporter.py`) that exposes model-specific metrics (e.g., prediction confidence, total requests).

## Docker-Based Deployment

![docker](Monitoring_dan_Logging/evidence/model-serving/docker-containers.png)

The project structure is intentionally modular, with each major component placed in a separate folder or container to ensure single-responsibility and ease of debugging. By isolating services, you can debug, restart, or redeploy only the component youâ€™re working on, without affecting the rest of the system.

Using Docker and Docker Compose, the architecture cleanly separates concerns:

* **`classifier/`**: Contains the model logic and FastAPI server to serve classification predictions. This is the core inference service.

**Monitoring Stack:**

* **`prometheus.yaml`**: Configuration for scraping metrics from services.
* **`grafana/`**: Dashboards and visualization setup.
* **`exporter/`**: Custom Prometheus exporter (e.g., for exposing internal metrics from the classifier).

### CI/CD Automation

The **CI/CD workflow** (`Workflow-CI.txt`) ensures that:

* Preprocessing scripts are automatically run on new commits.
* Cleaned and labeled data are updated in version control.
* All training artifacts are stored and tracked via MLflow (`Artifacts/`).
* Model versions can be deployed or served using DagsHub.

### Summary of Design Benefits

| Design Decision                       | Justification                                                        |
| ------------------------------------- | -------------------------------------------------------------------- |
| Sentence Embedding with Transformers  | Enables semantic understanding of resumes, improves generalization   |
| Modular Folder Structure              | Clear separation for model, API, monitoring, CI pipeline                   |
| Embedding API Separation              | Promotes reusability and independence of vector generation           |
| Caching Layer for Embeddings          | Boosts performance, reduces computation and API load                 |
| Prometheus + Grafana Monitoring Stack | Ensures system observability and alerts for production readiness     |
| Dockerized Microservices              | Facilitates reliable and reproducible deployment across environments |
| CI Integration                     | Guarantees data freshness, code quality, and fast iteration cycle    |

## Note & Recommended Learning Resources

To fully understand and extend this project, you should be familiar with the following tools and concepts:

### Tech Stack Used

- **Python** â€” Core scripting language for machine learning, APIs, and utilities.
- **[Scikit-learn](https://scikit-learn.org/)** â€” For building and evaluating the logistic regression model.
- **[Sentence-Transformers](https://www.sbert.net/)** â€” Pretrained semantic vectorizer (`all-MiniLM-L6-v2`) used to embed resume texts.
- **[FastAPI](https://fastapi.tiangolo.com/)** â€” Lightweight web framework for building the model inference API.
- **[Docker](https://www.docker.com/)** & **[Docker Compose](https://docs.docker.com/compose/)** â€” For containerizing services and orchestrating them.
- **[MLflow](https://mlflow.org/docs/latest/index.html)** â€” For experiment tracking, model registry, and artifact management.
- **[Prometheus](https://prometheus.io/)** & **[Grafana](https://grafana.com/)** â€” For system observability, metric collection, and real-time dashboards.
- **[GitHub Actions](https://docs.github.com/en/actions)** â€” CI/CD automation to run data preprocessing, testing, and model tracking pipelines.
- **[DagsHub](https://dagshub.com/)** â€” Optional platform for versioning data and managing ML pipelines remotely.

### Key Learning Materials

- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [FastAPI Docs](https://fastapi.tiangolo.com/)
- [Docker for Data Science](https://towardsdatascience.com/docker-for-data-science-5736a4caa2c2)
- [Monitoring ML with Prometheus and Grafana](https://www.valohai.com/blog/ml-monitoring-with-prometheus-and-grafana/)
- [Understanding Sentence Transformers](https://www.sbert.net/examples/applications/semantic-search/README.html)

> ğŸ“ **Note**: This project does **not** cover OCR, resume parsing, or GenAI summarization modules.